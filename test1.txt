RLHF是ChatGPT/InstrcutGPT实现与人类意图对齐，即按照人类指令尽可能生成无负面影响结果的重要技术[16]。该算法在强化学习框架下实现，大体可分为以下两个阶段：奖励模型训练该阶段旨在获取拟合人类偏好的奖励模型。奖励模型以提示和回复作为输入，计算标量奖励值作为输出。奖励模型的训练过程通过拟合人类对于不同回复的倾向性实现。具体而言，首先基于在人类撰写数据上精调的模型，针对同一提示采样多条不同回复。然后，将回复两两组合构成一条奖励模型训练样本，由人类给出倾向性标签。最终，奖励模型通过每条样本中两个回复的奖励值之差计算倾向性概率拟合人类标签，进而完成奖励模型的训练。生成策略优化给定习得的奖励模型，ChatGPT/InstructGPT的参数将被视为一种策略，在强化学习的框架下进行训练。首先，当前策略根据输入的查询采样回复。然后，奖励模型针对回复的质量计算奖励，反馈回当前策略用以更新。值得注意的是，为防止上述过程的过度优化，损失函数同时引入了词级别的KL惩罚项。此外，为了避免在公开NLP数据集上的性能退化，策略更新过程兼顾了预训练损失。