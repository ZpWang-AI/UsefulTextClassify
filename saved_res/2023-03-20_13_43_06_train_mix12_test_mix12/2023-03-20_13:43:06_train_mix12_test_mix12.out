[['base', True], ['batch_size', 8], ['clip', False], ['cuda_id', '8'], ['dev_data_file', ''], ['device', 'cuda'], ['epochs', 10], ['input_feature', 'reply only'], ['just_test', False], ['lr', 5e-05], ['model_name', 'hfl/chinese-roberta-wwm-ext'], ['pb_frequency', 20], ['pretrained_model_fold', './saved_model'], ['save_model_epoch', 1], ['save_res_fold', './saved_res'], ['train_data_file', './data/non_answer_dataset_for_zhipang.xlsx@./data/2_result.xlsx'], ['train_ratio', 0.8], ['version', 'train mix12 test mix12']]
2023-03-20_13:43:06
=== start training ===
batch[20/300], loss: 0.395416
batch[40/300], loss: 0.383191
batch[60/300], loss: 0.329712
batch[80/300], loss: 0.325523
batch[100/300], loss: 0.299224
batch[120/300], loss: 0.298099
batch[140/300], loss: 0.303800
batch[160/300], loss: 0.294063
batch[180/300], loss: 0.283942
batch[200/300], loss: 0.278304
batch[220/300], loss: 0.277619
batch[240/300], loss: 0.275589
batch[260/300], loss: 0.271649
batch[280/300], loss: 0.271101
batch[300/300], loss: 0.270498
epoch1 ends
f1       : 81.51%
accuracy : 91.00%
precision: 76.77%
recall   : 86.86%

	 pred_0	 pred_1
gt_0 	 427 	 36
gt_1 	 18 	 119
batch[20/300], loss: 0.101375
batch[40/300], loss: 0.115118
batch[60/300], loss: 0.145980
batch[80/300], loss: 0.156835
batch[100/300], loss: 0.153680
batch[120/300], loss: 0.149156
batch[140/300], loss: 0.151402
batch[160/300], loss: 0.157946
batch[180/300], loss: 0.158374
batch[200/300], loss: 0.165118
batch[220/300], loss: 0.160251
batch[240/300], loss: 0.166429
batch[260/300], loss: 0.167314
batch[280/300], loss: 0.166812
batch[300/300], loss: 0.164994
epoch2 ends
f1       : 82.56%
accuracy : 91.83%
precision: 80.56%
recall   : 84.67%

	 pred_0	 pred_1
gt_0 	 435 	 28
gt_1 	 21 	 116
batch[20/300], loss: 0.097984
batch[40/300], loss: 0.085088
batch[60/300], loss: 0.091513
batch[80/300], loss: 0.109601
batch[100/300], loss: 0.113160
batch[120/300], loss: 0.107743
batch[140/300], loss: 0.116114
batch[160/300], loss: 0.118467
batch[180/300], loss: 0.118923
batch[200/300], loss: 0.119429
batch[220/300], loss: 0.120395
batch[240/300], loss: 0.127340
batch[260/300], loss: 0.124216
batch[280/300], loss: 0.126880
batch[300/300], loss: 0.127639
epoch3 ends
f1       : 78.26%
accuracy : 90.83%
precision: 85.34%
recall   : 72.26%

	 pred_0	 pred_1
gt_0 	 446 	 17
gt_1 	 38 	 99
batch[20/300], loss: 0.106358
batch[40/300], loss: 0.111001
batch[60/300], loss: 0.103713
batch[80/300], loss: 0.088098
batch[100/300], loss: 0.077222
batch[120/300], loss: 0.081789
batch[140/300], loss: 0.079035
batch[160/300], loss: 0.082003
batch[180/300], loss: 0.079168
batch[200/300], loss: 0.081964
batch[220/300], loss: 0.084063
batch[240/300], loss: 0.084080
batch[260/300], loss: 0.084900
batch[280/300], loss: 0.087055
batch[300/300], loss: 0.086287
epoch4 ends
f1       : 80.28%
accuracy : 90.50%
precision: 76.32%
recall   : 84.67%

	 pred_0	 pred_1
gt_0 	 427 	 36
gt_1 	 21 	 116
batch[20/300], loss: 0.070246
batch[40/300], loss: 0.087580
batch[60/300], loss: 0.072861
batch[80/300], loss: 0.063319
batch[100/300], loss: 0.052356
batch[120/300], loss: 0.056657
batch[140/300], loss: 0.066327
batch[160/300], loss: 0.069051
batch[180/300], loss: 0.069275
batch[200/300], loss: 0.076496
batch[220/300], loss: 0.076889
batch[240/300], loss: 0.073010
batch[260/300], loss: 0.073192
batch[280/300], loss: 0.072599
batch[300/300], loss: 0.068444
epoch5 ends
f1       : 84.87%
accuracy : 93.17%
precision: 85.82%
recall   : 83.94%

	 pred_0	 pred_1
gt_0 	 444 	 19
gt_1 	 22 	 115
batch[20/300], loss: 0.004918
batch[40/300], loss: 0.010328
batch[60/300], loss: 0.010790
batch[80/300], loss: 0.018594
batch[100/300], loss: 0.029268
batch[120/300], loss: 0.059047
batch[140/300], loss: 0.057332
batch[160/300], loss: 0.053496
batch[180/300], loss: 0.054748
batch[200/300], loss: 0.055381
batch[220/300], loss: 0.058289
batch[240/300], loss: 0.064381
batch[260/300], loss: 0.061693
batch[280/300], loss: 0.058577
batch[300/300], loss: 0.060083
epoch6 ends
f1       : 80.00%
accuracy : 91.67%
precision: 88.50%
recall   : 72.99%

	 pred_0	 pred_1
gt_0 	 450 	 13
gt_1 	 37 	 100
batch[20/300], loss: 0.012750
batch[40/300], loss: 0.041916
batch[60/300], loss: 0.043333
batch[80/300], loss: 0.037797
batch[100/300], loss: 0.043771
batch[120/300], loss: 0.048659
batch[140/300], loss: 0.044823
batch[160/300], loss: 0.041122
batch[180/300], loss: 0.045689
batch[200/300], loss: 0.046705
batch[220/300], loss: 0.044243
batch[240/300], loss: 0.045508
batch[260/300], loss: 0.048167
batch[280/300], loss: 0.046488
batch[300/300], loss: 0.044751
epoch7 ends
f1       : 82.18%
accuracy : 91.83%
precision: 81.88%
recall   : 82.48%

	 pred_0	 pred_1
gt_0 	 438 	 25
gt_1 	 24 	 113
batch[20/300], loss: 0.035374
batch[40/300], loss: 0.026046
batch[60/300], loss: 0.027060
batch[80/300], loss: 0.030319
batch[100/300], loss: 0.025346
batch[120/300], loss: 0.026851
batch[140/300], loss: 0.024289
batch[160/300], loss: 0.026486
batch[180/300], loss: 0.034226
batch[200/300], loss: 0.036271
batch[220/300], loss: 0.037480
batch[240/300], loss: 0.038955
batch[260/300], loss: 0.040775
batch[280/300], loss: 0.038851
batch[300/300], loss: 0.037145
epoch8 ends
f1       : 80.29%
accuracy : 91.00%
precision: 80.29%
recall   : 80.29%

	 pred_0	 pred_1
gt_0 	 436 	 27
gt_1 	 27 	 110
batch[20/300], loss: 0.053399
batch[40/300], loss: 0.037095
batch[60/300], loss: 0.026593
batch[80/300], loss: 0.020910
batch[100/300], loss: 0.017115
batch[120/300], loss: 0.014495
batch[140/300], loss: 0.014025
batch[160/300], loss: 0.013974
batch[180/300], loss: 0.017797
batch[200/300], loss: 0.016480
batch[220/300], loss: 0.015113
batch[240/300], loss: 0.019762
batch[260/300], loss: 0.018528
batch[280/300], loss: 0.017332
batch[300/300], loss: 0.016257
epoch9 ends
f1       : 82.01%
accuracy : 91.67%
precision: 80.85%
recall   : 83.21%

	 pred_0	 pred_1
gt_0 	 436 	 27
gt_1 	 23 	 114
batch[20/300], loss: 0.000935
batch[40/300], loss: 0.001039
batch[60/300], loss: 0.004495
batch[80/300], loss: 0.004759
batch[100/300], loss: 0.008211
batch[120/300], loss: 0.010864
batch[140/300], loss: 0.011560
batch[160/300], loss: 0.025858
batch[180/300], loss: 0.032412
batch[200/300], loss: 0.033357
batch[220/300], loss: 0.033427
batch[240/300], loss: 0.034432
batch[260/300], loss: 0.039364
batch[280/300], loss: 0.039129
batch[300/300], loss: 0.037141
epoch10 ends
f1       : 81.40%
accuracy : 91.17%
precision: 78.38%
recall   : 84.67%

	 pred_0	 pred_1
gt_0 	 431 	 32
gt_1 	 21 	 116
=== finish training ===
2023-03-20_13:49:46
