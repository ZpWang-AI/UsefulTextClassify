[['base', True], ['batch_size', 8], ['clip', False], ['cuda_id', '8'], ['dev_data_file', ''], ['device', 'cuda'], ['epochs', 10], ['input_feature', 'reply only'], ['just_test', False], ['lr', 5e-05], ['model_name', 'hfl/chinese-roberta-wwm-ext'], ['pb_frequency', 20], ['pretrained_model_fold', './saved_model'], ['save_model_epoch', 1], ['save_res_fold', './saved_res'], ['train_data_file', './data/non_answer_dataset_for_zhipang.xlsx'], ['train_ratio', 0.8], ['version', 'train 1 test 1']]
2023-03-20_13:26:12
=== start training ===
batch[20/200], loss: 0.415850
batch[40/200], loss: 0.406550
batch[60/200], loss: 0.382322
batch[80/200], loss: 0.346909
batch[100/200], loss: 0.335148
batch[120/200], loss: 0.330670
batch[140/200], loss: 0.317756
batch[160/200], loss: 0.321377
batch[180/200], loss: 0.313023
batch[200/200], loss: 0.312796
epoch1 ends
f1       : 85.57%
accuracy : 93.00%
precision: 84.69%
recall   : 86.46%

	 pred_0	 pred_1
gt_0 	 289 	 15
gt_1 	 13 	 83
batch[20/200], loss: 0.194908
batch[40/200], loss: 0.172335
batch[60/200], loss: 0.162319
batch[80/200], loss: 0.171835
batch[100/200], loss: 0.185145
batch[120/200], loss: 0.192351
batch[140/200], loss: 0.184641
batch[160/200], loss: 0.184330
batch[180/200], loss: 0.181524
batch[200/200], loss: 0.189579
epoch2 ends
f1       : 74.25%
accuracy : 89.25%
precision: 87.32%
recall   : 64.58%

	 pred_0	 pred_1
gt_0 	 295 	 9
gt_1 	 34 	 62
batch[20/200], loss: 0.110840
batch[40/200], loss: 0.118718
batch[60/200], loss: 0.133158
batch[80/200], loss: 0.139993
batch[100/200], loss: 0.143629
batch[120/200], loss: 0.140783
batch[140/200], loss: 0.128760
batch[160/200], loss: 0.126857
batch[180/200], loss: 0.129377
batch[200/200], loss: 0.123351
epoch3 ends
f1       : 80.65%
accuracy : 91.00%
precision: 83.33%
recall   : 78.12%

	 pred_0	 pred_1
gt_0 	 289 	 15
gt_1 	 21 	 75
batch[20/200], loss: 0.144895
batch[40/200], loss: 0.096212
batch[60/200], loss: 0.081253
batch[80/200], loss: 0.071314
batch[100/200], loss: 0.096817
batch[120/200], loss: 0.104663
batch[140/200], loss: 0.102648
batch[160/200], loss: 0.106395
batch[180/200], loss: 0.112168
batch[200/200], loss: 0.116406
epoch4 ends
f1       : 81.05%
accuracy : 91.00%
precision: 81.91%
recall   : 80.21%

	 pred_0	 pred_1
gt_0 	 287 	 17
gt_1 	 19 	 77
batch[20/200], loss: 0.127960
batch[40/200], loss: 0.099702
batch[60/200], loss: 0.079877
batch[80/200], loss: 0.087016
batch[100/200], loss: 0.081094
batch[120/200], loss: 0.080527
batch[140/200], loss: 0.080227
batch[160/200], loss: 0.081450
batch[180/200], loss: 0.076864
batch[200/200], loss: 0.076048
epoch5 ends
f1       : 83.70%
accuracy : 92.50%
precision: 87.50%
recall   : 80.21%

	 pred_0	 pred_1
gt_0 	 293 	 11
gt_1 	 19 	 77
batch[20/200], loss: 0.066339
batch[40/200], loss: 0.057368
batch[60/200], loss: 0.050550
batch[80/200], loss: 0.043973
batch[100/200], loss: 0.050479
batch[120/200], loss: 0.051125
batch[140/200], loss: 0.058817
batch[160/200], loss: 0.059180
batch[180/200], loss: 0.057772
batch[200/200], loss: 0.061496
epoch6 ends
f1       : 83.06%
accuracy : 92.25%
precision: 87.36%
recall   : 79.17%

	 pred_0	 pred_1
gt_0 	 293 	 11
gt_1 	 20 	 76
batch[20/200], loss: 0.032188
batch[40/200], loss: 0.032026
batch[60/200], loss: 0.041955
batch[80/200], loss: 0.048103
batch[100/200], loss: 0.045012
batch[120/200], loss: 0.042876
batch[140/200], loss: 0.040699
batch[160/200], loss: 0.038317
batch[180/200], loss: 0.035793
batch[200/200], loss: 0.039263
epoch7 ends
f1       : 83.70%
accuracy : 92.50%
precision: 87.50%
recall   : 80.21%

	 pred_0	 pred_1
gt_0 	 293 	 11
gt_1 	 19 	 77
batch[20/200], loss: 0.138931
batch[40/200], loss: 0.089681
batch[60/200], loss: 0.065146
batch[80/200], loss: 0.057090
batch[100/200], loss: 0.046867
batch[120/200], loss: 0.039740
batch[140/200], loss: 0.047225
batch[160/200], loss: 0.049951
batch[180/200], loss: 0.070429
batch[200/200], loss: 0.078498
epoch8 ends
f1       : 84.31%
accuracy : 92.00%
precision: 79.63%
recall   : 89.58%

	 pred_0	 pred_1
gt_0 	 282 	 22
gt_1 	 10 	 86
batch[20/200], loss: 0.102523
batch[40/200], loss: 0.110771
batch[60/200], loss: 0.098407
batch[80/200], loss: 0.107881
batch[100/200], loss: 0.098844
batch[120/200], loss: 0.103436
batch[140/200], loss: 0.101778
batch[160/200], loss: 0.100085
batch[180/200], loss: 0.092089
batch[200/200], loss: 0.092989
epoch9 ends
f1       : 79.61%
accuracy : 89.50%
precision: 74.55%
recall   : 85.42%

	 pred_0	 pred_1
gt_0 	 276 	 28
gt_1 	 14 	 82
batch[20/200], loss: 0.086098
batch[40/200], loss: 0.076672
batch[60/200], loss: 0.068777
batch[80/200], loss: 0.066290
batch[100/200], loss: 0.067127
batch[120/200], loss: 0.066495
batch[140/200], loss: 0.059545
batch[160/200], loss: 0.054383
batch[180/200], loss: 0.052647
batch[200/200], loss: 0.050605
epoch10 ends
f1       : 85.71%
accuracy : 93.25%
precision: 87.10%
recall   : 84.38%

	 pred_0	 pred_1
gt_0 	 292 	 12
gt_1 	 15 	 81
=== finish training ===
2023-03-20_13:30:41
